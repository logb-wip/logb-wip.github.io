<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://logb-wip.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://logb-wip.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-08T15:09:38+00:00</updated><id>https://logb-wip.github.io/feed.xml</id><title type="html">blank</title><subtitle># A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">SAMformer - Efficient Time Series Forecasting with Transformers</title><link href="https://logb-wip.github.io/blog/2024/samformer/" rel="alternate" type="text/html" title="SAMformer - Efficient Time Series Forecasting with Transformers"/><published>2024-10-11T00:00:00+00:00</published><updated>2024-10-11T00:00:00+00:00</updated><id>https://logb-wip.github.io/blog/2024/samformer</id><content type="html" xml:base="https://logb-wip.github.io/blog/2024/samformer/"><![CDATA[<h2 id="goal-"><a id="goal"></a>Goal üöÄ</h2> <blockquote> <p>When a simple analysis leads to an efficient implementation.</p> </blockquote> <p>In this blog post, we focus on <strong>SAMformer</strong> proposed in <a href="https://arxiv.org/pdf/2402.10198"><em>SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting</em></a> <d-cite key="ilbert2024samformer"></d-cite>. SAMformer combines Sharpness-Aware Minimization (SAM) <d-cite key="foret2021sharpnessaware"></d-cite> and channel-wise attention to obtain a light-weight SOTA model with improved robustness and signal propagation compared to its competitors. This blog aims to provide a high-level view of the motivation behind SAMformer while explaining how to implement it. For the reader interested in more details, the paper is on <a href="https://arxiv.org/pdf/2402.10198">arXiv</a>, and the code can be found on <a href="https://github.com/romilbert/samformer">github</a>.</p> <h2 id="motivation-">Motivation üîé</h2> <p>Time series forecasting has many applications in real-world scenarios, e.g., anticipating cardiac arrhythmia in ECG signals, predicting electricity consumption to match future demand, or forecasting stock market prices (an exciting topic in times of inflation). This is notoriously challenging, especially in multivariate and long-term settings, due to feature correlations and long-term temporal dependencies in time series. Given its success on sequential data, many variants of the original transformer have been proposed, with computationally more efficient attention layers or well-engineered decomposition schemes to deal with the temporal dependencies. This led to a wide range of complex models with many parameters. However, it has been shown that linear models perform better than those SOTA Anything-formers <d-cite key="zeng2022transformerseffectivetimeseries"></d-cite>. This came as a surprise to us given the success of the Transformer architecture in NLP <d-cite key="brown2020fewshot"></d-cite> and Computer Vision <d-cite key="foret2021sharpnessaware"></d-cite> our study right up to the development of SAMformer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/dog_meme.PNG" sizes="95vw"/> <img src="/assets/img/blog_samformer/dog_meme.PNG" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="samformer-Ô∏è">SAMformer ‚öîÔ∏è</h2> <p>The process that led to SAMformer is akin to the good old-fashioned scientific method: empirical observations $\rightarrow$ hypothesis $\rightarrow$ experimental validation.</p> <p>1) Problem: transformers nuls en TS forecasting + very complicated and large-scale models ‚Äì&gt; hard to identify the failure. 2) We simplify transformer to only keep the key components 3) Problem identification: trainability issues 4) Possible solution: sigma reparam or SAM 5) SAM works ‚Äì&gt; putting evertyhing together 6) ‚ÄúOn va droit au but, allez voir le papier pour plus de detail.‚Äù (TO DO, something like ‚ÄúWe‚Äôll keep it concise, refer to the paper for more details.‚Äù).</p> <h3 id="trainability-issues-of-the-attention">Trainability Issues of the Attention</h3> <p>The Anything-formers are often complex and large, making pinpointing and addressing their weaknesses difficult. To identify the problem, we simplify the original Transformer <d-cite key="vaswani2017"></d-cite> to only keep the key components. Given that linear models outperformed more complex transformer ones, we naturally considered a toy problem of linear regression. The question is whether a simple transformer <em>tailored</em> to solve this task manages to do it, at least as well as a linear model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/sharpness_entropy_collapse_sam.pdf" sizes="95vw"/> <img src="/assets/img/blog_samformer/sharpness_entropy_collapse_sam.pdf" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="sam-to-the-rescue">SAM to the rescue</h3> <p>There are two possible solutions:</p> <ul> <li>$\sigma$-reparam <d-cite key="zhai2023sigmareparam"></d-cite>:</li> <li>SAM <d-cite key="foret2021sharpnessaware"></d-cite>:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/toy_exp_losses_val_all_methods_yaxis_modif.pdf" sizes="95vw"/> <img src="/assets/img/blog_samformer/toy_exp_losses_val_all_methods_yaxis_modif.pdf" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="putting-everything-together">Putting Everything Together</h3> <p>Now it works on our toy example: congrats you can now solve linear regression tasks. Hum, what about true time series data? We are only one step away from the optimal architecture: add revin <d-cite key="kim2022reversible"></d-cite></p> <p>In the end, SAMformer consists of 5 layers: RevIN normalization, channel-wise attention, residual connection, linear forecasting, and RevIN denormalization. And we are SOTA:</p> <p>fig: add table and/or result figure (e.g. generalization plots with stars).</p> <h2 id="getting-your-hands-dirty-Ô∏è">Getting your hands dirty üñ•Ô∏è</h2> <p>In this section, we discuss the implementation of SAMformer which makes use of modern deep learning frameworks such as <code class="language-plaintext highlighter-rouge">PyTorch</code> or <code class="language-plaintext highlighter-rouge">TensorFlow</code> and can be found <a href="https://github.com/romilbert/samformer">here</a>.</p> <h3 id="main-components">Main Components</h3> <p>As can be seen below, SAMformer consists of 5 layers:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/samformer_arch.png" sizes="95vw"/> <img src="/assets/img/blog_samformer/samformer_arch.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It leads to a shallow transformer with a single head and a single encoder that can be trained with SAM <d-cite key="foret2021sharpnessaware"></d-cite>.</p> <p>We provide a snippet of SAMformer (few) code lines below for the interested reader.</p> <details><summary>SAMformer Implementation</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SAMFormerArchitecture</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">revin</span> <span class="o">=</span> <span class="nc">RevIN</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_keys</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_queries</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_values</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_forecaster</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1"># RevIN Normalization
</span>        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">norm</span><span class="sh">'</span><span class="p">)</span> 
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x_norm</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Channel-Wise Attention
</span>        <span class="n">queries</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_queries</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">keys</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_keys</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>        <span class="n">att_score</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Residual Connection
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">att_score</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Linear Forecasting
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_forecaster</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="c1"># RevIN Denormalization
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">denorm</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> </details> <h2 id="future-work">Future Work</h2> <p>While studying the trainability issues of the Transformer, we stumbled over the fact that the entropy collapse appeared in tandem with a sharp loss. However, we observed that the entropy collapse was benign, i.e., solving it did not improve the performance that much. This is different from the conclusions on text and images found in <d-cite key="zhai2023sigmareparam"></d-cite>. Moreover, we saw that using $\sigma$-reparam <d-cite key="zhai2023sigmareparam"></d-cite> decreases the signal propagation a lot, up to the point of having almost uniform attention. This leads to rank collapse, which is known to appear in attention-based models and to impact the generalization performance <d-cite key="anagnostidis2022signal"></d-cite> <d-cite key="dong2021attentionrank"></d-cite>. Finally, we manage to demonstrate that, indeed, $\sigma$-reparam induces a rank collapse. Formally, $\sigma$-reparam aims to minimize blabla, which in turn can be used to upper-bound the rank of the attention internal computations. We have</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/nuclear_norm.png" sizes="95vw"/> <img src="/assets/img/blog_samformer/nuclear_norm.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Sigma reparam bla bla (citer Sinkformer <d-cite key="sander2022sinkformer"></d-cite> + rank and signal propagation work on attention (attention is not all u need + signal propagation in transformer).</p> <h2 id="conclusion">Conclusion</h2> <p>This blog post has presented Ambroise‚Äôs recent work on transformers for time series forecasting. By integrating SAM with channel-wise attention, SAMformer achieves state-of-the-art performance with a lightweight and robust design, making it a superior choice for time series forecasting. While there are many models and now foundation models for time series forecasting, SAMformer has the benefit of simplicity and efficiency. Beyond the model itself, we felt it was important to share the research process that led to it in a world of rapid changes and extreme development of algorithms that are performance-oriented. While SAMformer outperforms SOTA models (at the time) for a fraction of the computational cost, our goal was primarily to better understand the failure modes of previous methods. We hope this kind of methodology can inspire researchers and practitioners towards more efficient and reliable methods.</p> <p>Finally, we want to acknowledge and thank the open-source community. In particular, this work would not have been possible without access to the code and papers of SAM[], $\sigma$-reparam[] or to the machine learning libraries such as PyTorch, TensorFlow, or Scikit-Learn. For the reader interested in more mathematical details or getting started with SAMformer, the paper is on <a href="https://arxiv.org/pdf/2402.10198">arXiv</a>, and the code can be found on <a href="https://github.com/romilbert/samformer">github</a>.</p> <h2 id="acknowledgments-"><a id="acknowledgments"></a>Acknowledgments üôèüèæ</h2> <p>We thank TBD for taking the time to proofread this blog post. Ambroise thanks all his co-authors, without whom SAMformer <d-cite key="ilbert2024samformer"></d-cite> wouldn‚Äôt exist, and in particular, his Ph.D. supervisor, Ievgen Redko, for the freedom and trust he provided during this project.</p> <p>For any further questions, please feel free to leave a comment or contact us by mail!</p>]]></content><author><name>Ambroise Odonnat</name></author><category term="transformers"/><category term="deep learning"/><category term="time series forecasting"/><category term="maths"/><category term="code"/><summary type="html"><![CDATA[Improved attention and optimization for better performance]]></summary></entry><entry><title type="html">Kernel Trick I - Deep Convolutional Representations in RKHS</title><link href="https://logb-wip.github.io/blog/2024/ckn/" rel="alternate" type="text/html" title="Kernel Trick I - Deep Convolutional Representations in RKHS"/><published>2024-07-18T00:00:00+00:00</published><updated>2024-07-18T00:00:00+00:00</updated><id>https://logb-wip.github.io/blog/2024/ckn</id><content type="html" xml:base="https://logb-wip.github.io/blog/2024/ckn/"><![CDATA[<h2 id="goal-"><a id="goal"></a>Goal üöÄ</h2> <blockquote> <p>Fear not, those who delved into the maths of the kernel trick, for its advent in deep learning is coming.</p> </blockquote> <p>In this blog post, we focus on the <strong><em>Convolutional Kernel Network</em></strong> (CKN) architecture proposed in <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf"><em>End-to-End Kernel Learning with Supervised Convolutional Kernel Networks</em></a> <d-cite key="mairal2016endtoend"></d-cite> and present its guiding principles and main components. The CKN opened a new line of research in deep learning by demonstrating the benefits of the kernel trick for deep convolutional representations. The goal of this blog is to provide a high-level view of the CKN architecture while explaining how to implement it from scratch without relying on modern Deep Learning frameworks. For a more complete picture from the mathematical side, we invite the reader to refer to the original paper <d-cite key="mairal2016endtoend"></d-cite>.</p> <h2 id="kernel-trick-Ô∏è">Kernel Trick üßô‚Äç‚ôÇÔ∏è</h2> <p>Before diving into the CKN itself, we briefly recall what the <em>kernel trick</em> stands for. In many situations, one needs to measure the similarity between inputs. Such inputs would typically be mapped to a high-dimensional space and compared by computing their inner product. More formally, place yourself in your favorite high-dimensional feature space \(\mathcal{H}\) and consider a mapping \(\varphi\colon \mathcal{X} \to \mathcal{H}\). The similarity between two inputs \(\mathbf{x}, \mathbf{x}'\) the inner product \(\langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}}\). The kernel trick enables us to avoid working with high-dimensional vectors in \(\mathcal{H}\) and directly evaluate such pairwise comparison by kernel evaluation, i.e.,</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}} \end{equation*}\] <p>The kernel trick presents many advantages. One of whom is that, in practice, one can choose a kernel \(K\) such that the equation above holds for some map \(\varphi\), without needing to obtain \(\varphi\) in closed form. The existence of such kernels has been established in the rich literature on kernel methods. For the interested reader, more details can be found in <d-cite key="scholkopf2000kerneltrick"></d-cite> or in the <a href="https://mva-kernel-methods.github.io/course-2023-2024/static_files/materials/slides.pdf">slides</a> of <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a> and <a href="https://jpvert.github.io/">Jean-Philippe Vert</a> that contains all one needs to know about kernel methods. For the sake of self-consistency, we give an example of a well-known class of kernels for which the kernel trick can be applied.</p> <details><summary>Positive definite kernels :mag_right:</summary> <p>One of the most prominent examples of kernels for which the kernel trick can be applied is the class of positive definite (p.d.) kernels. Formally, a p.d. kernel is a function \(K : \mathcal{X}\times\mathcal{X} \to \mathbb{R}\) that is symmetric, i.e.,</p> \[\forall \mathbf{x},\mathbf{x}', ~K(\mathbf{x},\mathbf{x}') = K(\mathbf{x}',\mathbf{x}),\] <p>and verifies</p> \[\forall (\mathbf{x}_1, \dots, \mathbf{x}_N) \in \mathcal{X}^N, (\alpha_1, \dots, \alpha_N) \in \mathbb{R}^N, \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j K(\mathbf{x}_i,\mathbf{x}_j) \geq 0.\] <p>In practice, assuming one wants to work with vectors of \(\mathbb{R}^d\), a p.d. kernel can be easily obtained by considering a mapping \(\varphi \colon \mathcal{X} \to \mathbb{R}^d\) and defining \(K \colon \mathcal{X}^2 \to \mathbb{R}\) as</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}}. \end{equation*}\] <p>Surprisingly, the converse is also true, i.e., any p.d. kernel can be expressed as an inner product. More formally, the Aronszajn Theorem <d-cite key="aronszajn1950reproducing"></d-cite> states that \(K\) is a p.d. kernel on the set \(\mathcal{x}\) <strong>if and only if</strong> there exists a Hilbert Space \(\mathcal{H}\) and a mapping \(\varphi \colon \mathcal{X} \to \mathcal{H}\) such that</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}}. \end{equation*}\] </details> <p>One of the most direct and popular applications of the kernel trick is with Support Vector Machines (SVMs) where similarities are measured by a dot product in a high-dimensional feature space. Using the kernel trick makes the classification task easier and <strong><em>only requires kernel pairwise evaluation</em></strong> instead of explicitly manipulating high-dimensional vectors. The short animation below shows how the kernel trick can be used to map tangled data originally lying on a 2D plane to a higher dimensional space where they can be separated by a hyperplane.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/kernel_trick_colored.gif" sizes="95vw"/> <img src="/assets/img/blog_ckn/kernel_trick_colored.gif" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It goes without saying that the range of applications of the kernel trick is large and goes beyond SVMs and making data linearly separable. In the rest of this post, we study how the kernel trick can be used to derive a new type of convolutional neural network that couples data representation and prediction tasks.</p> <h2 id="convolutional-kernel-network-in-depth-"><a id="convolutional-kernel-network-in-depth"></a>Convolutional Kernel Network in-depth üîé</h2> <p>Now that everyone has a clear head regarding the kernel trick, we are prepared to introduce the so-called <strong><em>Convolutional Kernel Network</em></strong>. The main idea behind the CKN is to leverage the kernel trick to represent local neighborhoods of images. In this section, we explain how the authors build Convolutional Kernel layers that can be stacked into a multi-layer CKN.</p> <h3 id="motivation-representing-local-image-neighborhoods">Motivation: representing local image neighborhoods</h3> <p>The CKN architecture is a type of convolutional neural network that couples the prediction task with representation learning. The main novelty is to benefit from the kernel trick to learn nonlinear representations of local image patches. Formally, taking into account the $3$ color channels, an image can be described as a mapping \(I_0 \colon \Omega_0 \to \mathbb{R}^{3}\) where \(\Omega_0 \subset [0,1]^2\) is a set of pixel coordinates. Reusing the notations previously introduced, we can consider a p.d. kernel \(K\colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) that is implicitly associated to a Hilbert space \(\mathcal{H}\), called the Reproducing Kernel Hilbert Space (RKHS), and a mapping \(\varphi \colon \mathcal{X} \to \mathcal{H}\). Considering two patches \(\mathbf{x}\) and \(\mathbf{x}'\) extracted from \(I_0\), their representation in \(\mathcal{H}\) is simply given by \(\varphi(\mathbf{x})\) and \(\varphi(\mathbf{x}')\). By definition, this embedding verifies</p> \[\begin{equation*} K(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}')\rangle_{\mathcal{H}} \end{equation*}.\] <p>The original CKN paper <d-cite key="mairal2016endtoend"></d-cite> uses a specific type of p.d. kernels, namely <em>homogeneous dot-product</em> kernels \(K \colon \mathcal{X}\times\mathcal{X} \to \mathbb{R}\) that verify</p> \[\begin{equation*} \forall \mathbf{x},\mathbf{x}' \in \mathcal{X},~ K(\mathbf{x},\mathbf{x}') = \lVert \mathbf{x} \rVert\lVert \mathbf{x}'\rVert \kappa \left(\left\langle \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert},\frac{\mathbf{x}'}{\lVert \mathbf{x}'\rVert}\right\rangle\right). \end{equation*}\] <p>The function \(\kappa(\langle \cdot, \cdot \rangle)\) should be a smooth dot-product kernel on the sphere whose Taylor expansion has non-negative coefficients to ensure positive definiteness. A common example of such dot-product kernel is the exponential kernel that verifies for inputs \(\mathbf{y}, \mathbf{y}'\) with unit \(\ell_2\) norm</p> \[\begin{equation*} \kappa_{\exp}(\mathbf{y},\mathbf{y}') = \text{e}^{\beta(\langle \mathbf{y},\mathbf{y}'\rangle -1)} = \text{e}^{-\frac{\beta}{2}\lVert \mathbf{y}-\mathbf{y}' \rVert_2^2} \end{equation*},\] <p>for \(\beta &gt; 0\). Taking \(\displaystyle\beta = \frac{1}{\sigma^2}\) leads to the well-known Gaussian (RBF) Kernel. We will see below that the parameter \(\beta\) can be learned along the training of the neural network.</p> <h3 id="from-theory-to-practice">From theory to practice</h3> <p>While the kernel trick is elegant and appealing from a mathematical perspective, one can wonder how to implement it in practice. Indeed, the RKHS \(\mathcal{H}\) can be of <strong><em>infinite</em></strong> dimension which makes it computationally intractable :dizzy_face:! Fortunately, our friend Nystr√∂m comes to the rescue to approximate the feature representations \(\varphi(\mathbf{x})\) and \(\varphi(\mathbf{x}')\) by their projection \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\) onto a <strong><em>finite</em></strong> dimensional subspace \(\mathcal{F}\) (see the figure below).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/Nystrom.png" sizes="95vw"/> <img src="/assets/img/blog_ckn/Nystrom.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The subspace \(\mathcal{F}\) is defined as \(\mathcal{F} = \text{span}(z_1,\dots,z_p)\), where the \((z_i)_{i\in\{1\dots p\}}\) are anchor points of unit-norm. As explained in <d-cite key="mairal2016endtoend"></d-cite>, the subspace \(\mathcal{F}\) can be optimized in both a <em>supervised</em> (with backpropagation rules) or an <em>unsupervised</em> way (by minimizing projection residuals with spherical K-Means). It should be noted that for a given layer of the CKN architecture, \(\mathbf{Z} = \{z_1,\dots,z_p\}\) refer to the parameters of this layer which means that \(\mathcal{F}\) changes along the training. In particular, as said above, if the RBF kernel is considered, the parameter \(\beta\) is optimized along the training which avoids having to tune it.</p> <h3 id="building-a-convolutional-kernel-layer">Building a Convolutional Kernel layer</h3> <p>Putting everything together, a Convolutional Kernel Layer can be built in three steps.</p> <ul> <li>Extract patches \(\mathbf{x}\) from the image \(I_0\). <details> <summary>Patch Extraction</summary> <pre><code class="language-python">def extract_2d_patches(self, x):
      h, w = self.filter_size
      batch_size, C, _, _ = x.shape
      unfolded_x = np.lib.stride_tricks.sliding_window_view(x, (batch_size, C, h, w))
      unfolded_x = unfolded_x.reshape(-1, self.patch_dim)
      return unfolded_x
  
  def sample_patches(self, x_in, n_sampling_patches=1000):
      patches = self.extract_2d_patches(x_in)
      n_sampling_patches = min(patches.shape[0], n_sampling_patches)
      patches = patches[:n_sampling_patches]
      return patches</code></pre></details> </li> <li>Normalize and convolve them as \(\lVert \mathbf{x} \rVert \kappa \left( \mathbf{Z}^\top \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||} \right)\) and compute the approximation as \(\psi(x) = \lVert \mathbf{x} \rVert\kappa\left(\mathbf{Z}^\top \mathbf{Z}\right)^{-1/2}\kappa\left(\mathbf{Z}^\top \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||}\right)\) by applying the linear transform \(\kappa\left(\mathbf{Z}^\top \mathbf{Z}\right)^{-1/2}\) at every pixel location, <details> <summary>Convolutional Layer</summary> <pre><code class="language-python">def conv_layer(self, x_in):
      patch_norm = np.sqrt(np.clip(conv2d_scipy(x_in**2, self.ones, bias=None,
              stride=1, padding=self.padding, dilation=self.dilation,
              groups=self.groups), a_min=EPS, a_max=None))
      x_out = conv2d_scipy(x_in, self.weight, self.bias, (1,1),
                      self.padding, self.dilation, self.groups)
      x_out = x_out / np.clip(patch_norm, a_min=EPS, a_max=None)
      x_out = patch_norm * self.kappa(x_out)
      return x_out
  
  def mult_layer(self, x_in, lintrans):
      batch_size, in_c, H, W = x_in.shape
      x_out = np.matmul(
          np.tile(lintrans, (batch_size, 1, 1)), 
          x_in.reshape(batch_size, in_c, -1))
      return x_out.reshape(batch_size, in_c, H, W)</code></pre></details> </li> <li> <p>Apply pooling operations. Note that Gaussian linear pooling is defined as</p> \[\begin{equation*} \displaystyle I_1(x) = \int_{\mathbf{x}'\in\Omega_0} M_1(x') \text{e}^{-\beta\lVert \mathbf{x}-\mathbf{x}'\rVert_2^2}\text{d}\mathbf{x}' \end{equation*}\] <p>where \(M_1\) is the ‚Äúfeature map‚Äù after the second point operation. That is why, we can interpret the pooling operation as a ‚Äúconvolution‚Äù operation. </p> <details> <summary>Pooling Layer</summary> <pre><code class="language-python">def pool_layer(self, x_in):
      if self.subsampling &lt;= 1:
          return x_in
      x_out = conv2d_scipy(x_in, self.pooling_filter, bias=None, 
          stride=self.subsampling, padding=self.subsampling, 
          groups=self.out_channels)
      return x_out</code></pre></details> </li> </ul> <p>The figure below summarizes all those operations, providing an overview of a Convolutional Kernel layer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CKN.png" sizes="95vw"/> <img src="/assets/img/blog_ckn/CKN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="multi-layer-ckn">Multi-layer CKN</h3> <p>The first principles presented above enable to obtain a ‚Äúfeature map‚Äù \(I_1 \colon \Omega_1 \to \mathbb{R}^{p_1}\) from the original image \(I_0 \colon \Omega_0 \to \mathbb{R}^{3}\). Applying the same procedure leads to another map \(I_2 \colon \Omega_2 \to \mathbb{R}^{p_2}\), and another map \(I_3 \colon \Omega_3 \to \mathbb{R}^{p_3}\), and so on and so forth. In summary, a multilayer CKN consists of stacking multiple Convolutional Kernel layers. It should be noted that similarly to the convolutional neural network (CNN), the \(I_k \in \mathbb{R}^k\) represent larger and larger image neighborhoods with \(k\) increasing, gaining more invariance thanks to the pooling layers.</p> <h2 id="cnn-vs-ckn-Ô∏è">CNN vs. CKN ‚öîÔ∏è</h2> <p>In this part, we recall the main differences between the vanilla convolutional neural network (CNN) and the convolutional kernel network (CKN). It should be noted that the CKN is a type of CNN where the representation learning relies on the kernel trick.</p> <h3 id="overview">Overview</h3> <p>In <d-cite key="bietti2018group"></d-cite>, it is shown that CKNs contain a large class of CNNs with smooth homogeneous activation functions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_ckn/CNN.png" sizes="95vw"/> <img src="/assets/img/blog_ckn/CNN.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The similarities and differences between CKN and CNN are well illustrated in the two previous figures.</p> <p>On the one hand, A CNN of \(L\) layer can be represented by its output \(f_{\text{CNN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as</p> \[\begin{equation*} f_{\text{CNN}}(\mathbf{x}) = \gamma_L(\sigma_L(W_L\dots \gamma_2(\sigma_2(W_2\gamma_1(\sigma_1(W_1\mathbf{x}))\dots)), \end{equation*}\] <p>where \((W_k)_k\) represent the convolution operations, \((\sigma_k)_k\) are pointwise non-linear functions, (e.g., ReLU), and \((\gamma_k)_k\) represent the pooling operations (see <d-cite key="paulin2016convolutional"></d-cite>).</p> <p>On the other hand, A CKN of \(L\) layer can be represented by its output \(f_{\text{CKN}}(\mathbf{x})\), if \(\mathbf{x}\) is the input, as</p> \[\begin{equation*} f_{\text{CKN}}(\mathbf{x}) = \gamma_L(\sigma_L(W_L(P_L\dots \gamma_2(\sigma_2(W_2(P_2(\gamma_1(\sigma_1(W_1(P_1(\mathbf{x}))\dots)), \end{equation*}\] <p>where \((P_{k})_{k}\) represent the patch extractions, \((W_{k})_{k}\) the convolution operations, \((\sigma_{k})_{k}\) the kernel operations (which allows us to learn non-linearity in the RKHS), and \((\gamma_{k})_{k}\) the pooling operations.</p> <h2 id="getting-your-hands-dirty-Ô∏è">Getting your hands dirty üñ•Ô∏è</h2> <p>In this section, we discuss the implementation of the CKN architecture and show how to reimplement it from scratch.</p> <h3 id="modern-implementation">Modern Implementation</h3> <p>The original implementation of the CKN architecture makes use of modern deep learning frameworks such as <code class="language-plaintext highlighter-rouge">PyTorch</code>, <code class="language-plaintext highlighter-rouge">TensorFlow</code> or <code class="language-plaintext highlighter-rouge">JAX</code> and can be found <a href="https://github.com/claying/CKN-Pytorch-image">here</a>. We recommend using it if the performance is at stake.</p> <h3 id="stonage-ml">Stonage ML</h3> <p>To better understand how things work, we decided to reimplement the CKN architecture without using modern Deep Learning frameworks. It saves you the trouble of reading hundreds of documentation pages, but in return, the computational efficiency is worse for large-scale applications. Our open-source implementation of the full CKN architecture from scratch can be found <a href="https://github.com/ozekri/CKN_from_Scratch">here</a>.</p> <h4 id="autodiff">Autodiff</h4> <p>Automatic differentiation (autodiff) is a well-known algorithm that is absolutely essential in deep learning. It allows us to update the parameters of a network, by computing the derivatives with the chain rule thanks to a computational graph. If you want to implement from scratch this algorithm, you will have to implement from scratch an efficient computational graph.</p> <div style="display: flex; justify-content: center;"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Backprop in neural networks is reverse mode auto-diff applied to a simple linear computational graph. The simplicity of the resulting algorithm somehow overshadows the power and non-triviality of applying auto-diff to complex (e.g. recursive) graphs. <a href="https://t.co/5op8P7oYrE">https://t.co/5op8P7oYrE</a> <a href="https://t.co/ETafufgGqa">pic.twitter.com/ETafufgGqa</a></p>&mdash; Gabriel Peyr√© (@gabrielpeyre) <a href="https://twitter.com/gabrielpeyre/status/956932467092574213?ref_src=twsrc%5Etfw">January 26, 2018</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div> <p>For implementation from scratch details of autodiff, see this really simple <a href="https://e-dorigatti.github.io/math/deep%20learning/2020/04/07/autodiff.html">blog post</a> from <a href="https://e-dorigatti.github.io/">Emilio Dorigatti</a>‚Äôs website. But let‚Äôs go back to our CKN now.</p> <p>Given a training set of \(n\) images \((I_{0}^1\), \(I_{0}^2, \ldots, I_{0}^n)\), optimizing a CKN of \(L\) layers consists of jointly minimizing the following optimization problem with respect to \(\mathbf{W} \in \mathbb{R}^{p_L \times \lvert\Omega_L\rvert}\) and with respect to the set of filters \(\mathbf{Z}_1,\ldots,\mathbf{Z}_L\)</p> \[\begin{equation*} \min_{\begin{aligned}&amp;\mathbf{W} \in \mathbb{R}^{p_L \times \lvert\Omega_L\rvert}\\ &amp;\mathbf{Z}_1,\ldots,\mathbf{Z}_L\end{aligned}} \frac{1}{n} \sum_{i=1}^n \mathcal{L}( y_i, \langle \mathbf{W} , I_{L}^i \rangle ) + \frac{\lambda}{2} \lVert \mathbf{W} \rVert_{\text{F}}^2 \end{equation*}\] <p>where \(\mathcal{L}\) is a loss function, \(\lVert \cdot \rVert_{\text{F}}\) is the Frobenius norm that extends the Euclidean norm to matrices, and, with abuse of notation, the maps \(I_{k}^i\) are seen as matrices in \(\mathbb{R}^{p_L \times \lvert\Omega_L\rvert}\).</p> <p>Optimizing with respect to \(\mathbf{W}\) is straightforward with any gradient-based method.</p> <p>Optimizing with respect to the \(Z_j, j \in \{1,\ldots, L\}\) is a bit more tricky, as we have to examine the quantity \(\nabla_{\mathbf{Z}_{j}} \mathcal{L} (y_i, \langle \mathbf{W} , I_{L}^i \rangle)\), for \(j \in \{1,\ldots, L\}\) to compute the derivative. Once it is done, we can just use autodiff. The formulation of the chain rule is not straightforward, please check <d-cite key="mairal2016endtoend"></d-cite> to see the technical details.</p> <h4 id="convolutional-operations">Convolutional operations</h4> <p>Little-known fact: <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="language-plaintext highlighter-rouge">torch.conv2D</code></a> computes a <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, not a <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> (yeah, the name is confusing! :weary:). But what is the difference? The discrete convolution operator \(\ast\) between two functions \(g\) and \(h\) is defined as</p> \[\begin{equation*} (g \ast h)[n]=\sum_{m=-\infty}^{\infty} g[m] h[n-m] \end{equation*}\] <p>whereas the discrete cross-correlation operator \(\circledast\) is defined as</p> \[\begin{equation*} (g \circledast h)[n]=\sum_{m=-\infty}^{\infty} \overline{g[m]} h[n+m] \end{equation*}\] <p>where \(\overline{g[m]}\) denotes the complex conjugate of \(g[m]\). It‚Äôs subtle, but in the case of images, cross-correlation requires one less ‚Äúimage flipping‚Äù than convolution because of the minus sign in \(h[n-m]\). Given the number of convolutions we‚Äôre going to calculate, if we can spare ourselves an ‚Äúimage flip‚Äù each time, we‚Äôll do it! What‚Äôs more, as the filter parameters are learnable, it doesn‚Äôt matter if we choose cross-correlation rather than convolution.</p> <p>That being said, let‚Äôs underline the fact that accelerating the correlation operations is crucial, as it is very much part of the framework. In fact, we use correlations for convolutional layers, but we‚Äôve also implemented linear pooling as a correlation! These operations will be performed so frequently that, with an implementation from scratch, it is <strong>absolutely essential to parallelize the process</strong>, to make it run much faster on GPUs.</p> <h4 id="parallelization-Ô∏è">Parallelization ‚õ∑Ô∏è</h4> <p>We‚Äôre not allowed to use <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html"><code class="language-plaintext highlighter-rouge">torch.nn.functional.conv2d</code></a>, so to parallelize <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html#scipy.signal.correlate2d"><code class="language-plaintext highlighter-rouge">scipy.signal.correlate2d</code></a>, we‚Äôre left with two solutions.</p> <p><u>1. Use CuPy!</u></p> <p><a href="https://cupy.dev/"><code class="language-plaintext highlighter-rouge">CuPy</code></a> is an open-source library for GPU-accelerated computing with Python programming language. <code class="language-plaintext highlighter-rouge">CuPy</code> shares the same API set as <code class="language-plaintext highlighter-rouge">NumPy</code> and <code class="language-plaintext highlighter-rouge">SciPy</code>, allowing it to be a drop-in replacement to run NumPy/SciPy code on GPU.</p> <p>On top of that, <code class="language-plaintext highlighter-rouge">CuPy</code> has its own implementation of <code class="language-plaintext highlighter-rouge">scipy.signal.correlate2d</code> <a href="https://docs.cupy.dev/en/latest/reference/generated/cupyx.scipy.signal.correlate2d.html">here</a>, and it performs superbly. See the code below for a comparison with the original one, and <code class="language-plaintext highlighter-rouge">torch.nn.functional.conv2d</code>.</p> <details><summary>Execution Time</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">timeit</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">cupy</span> <span class="k">as</span> <span class="n">cp</span>
<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
<span class="kn">from</span> <span class="n">cupyx.scipy.signal</span> <span class="kn">import</span> <span class="n">correlate2d</span> <span class="k">as</span> <span class="n">cupyx_correlate2d</span>

<span class="c1"># Creating a test image and kernel
</span><span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Correlation calculation with scipy.signal.correlate2d
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span>
<span class="n">result_scipy</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="nf">correlate2d</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">valid</span><span class="sh">'</span><span class="p">)</span>
<span class="n">scipy_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Execution time with scipy.signal.correlate2d :</span><span class="sh">"</span><span class="p">,</span> <span class="n">scipy_time</span><span class="p">)</span>

<span class="c1"># Correlation calculation with torch.nn.functional.conv2d
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span>
<span class="n">result_torch</span> <span class="o">=</span> <span class="nf">torch_correlate2d</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">valid</span><span class="sh">'</span><span class="p">)</span>
<span class="n">torch_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Execution time with torch.nn.functional.conv2d :</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch_time</span><span class="p">)</span>

<span class="c1"># Correlation calculation with cupyx.scipy.signal.correlate2d
</span><span class="n">image_gpu</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">kernel_gpu</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span>
<span class="n">result_cupyx</span> <span class="o">=</span> <span class="nf">cupyx_correlate2d</span><span class="p">(</span><span class="n">image_gpu</span><span class="p">,</span> <span class="n">kernel_gpu</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">valid</span><span class="sh">'</span><span class="p">)</span>
<span class="n">cupyx_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">.</span><span class="nf">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Execution time with cupyx.scipy.signal.correlate2d :</span><span class="sh">"</span><span class="p">,</span> <span class="n">cupyx_time</span><span class="p">)</span></code></pre></figure> </details> <p>Running the code above produces the following output:</p> <pre><code class="language-python">scipy.signal.correlate2d : 8.6376 seconds
torch.nn.functional.conv2d : 0.1617 seconds
cupyx.scipy.signal.correlate2d : 0.0006 seconds</code></pre> <p>Note that there also exist a <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.signal.correlate2d.html">LAX-backend implementation</a> of <code class="language-plaintext highlighter-rouge">scipy.signal.correlate2d</code> in <code class="language-plaintext highlighter-rouge">JAX</code>.</p> <p><u>2. Use a Low-Level Language</u></p> <p>You can also implement the function in a low-level language such as C or C++ for better performance, and use a high-level language like Python to call this implementation. In practice, this is what is done in PyTorch. In our work, we re-implemented the <code class="language-plaintext highlighter-rouge">scipy.signal.correlate2d</code> using Nvidia CUDA. We provide below the corresponding implementation.</p> <details><summary>CUDA Implementation</summary> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">extern</span> <span class="s">"C"</span> <span class="p">{</span>
    <span class="n">__global__</span> <span class="kt">void</span> <span class="n">correlate2d_gpu_kernel</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">result</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">image</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">kernel</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">image_width</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">image_height</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">kernel_width</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">kernel_height</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">image_width</span> <span class="o">-</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ki</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ki</span> <span class="o">&lt;</span> <span class="n">kernel_width</span><span class="p">;</span> <span class="n">ki</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">kj</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">kj</span> <span class="o">&lt;</span> <span class="n">kernel_height</span><span class="p">;</span> <span class="n">kj</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="n">sum</span> <span class="o">+=</span> <span class="n">kernel</span><span class="p">[</span><span class="n">ki</span> <span class="o">*</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="n">kj</span><span class="p">]</span> <span class="o">*</span> <span class="n">image</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="n">ki</span><span class="p">)</span> <span class="o">*</span> <span class="n">image_width</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="n">kj</span><span class="p">)];</span>
                <span class="p">}</span>
            <span class="p">}</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kt">void</span> <span class="nf">correlate2d_gpu</span><span class="p">(</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">result</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">image</span><span class="p">,</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">kernel</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">image_width</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">image_height</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">kernel_width</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">kernel_height</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">d_result</span><span class="p">;</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">d_image</span><span class="p">;</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">d_kernel</span><span class="p">;</span>

        <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_result</span><span class="p">,</span> <span class="p">(</span><span class="n">image_width</span> <span class="o">-</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
        <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_image</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">*</span> <span class="n">image_height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
        <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_kernel</span><span class="p">,</span> <span class="n">kernel_width</span> <span class="o">*</span> <span class="n">kernel_height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

        <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_image</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">*</span> <span class="n">image_height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
        <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_kernel</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">kernel_width</span> <span class="o">*</span> <span class="n">kernel_height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

        <span class="n">dim3</span> <span class="n">blockSize</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">);</span>
        <span class="n">dim3</span> <span class="n">gridSize</span><span class="p">((</span><span class="n">image_width</span> <span class="o">-</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>

        <span class="n">correlate2d_gpu_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_result</span><span class="p">,</span> <span class="n">d_image</span><span class="p">,</span> <span class="n">d_kernel</span><span class="p">,</span> <span class="n">image_width</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">kernel_width</span><span class="p">,</span> <span class="n">kernel_height</span><span class="p">);</span>

        <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">d_result</span><span class="p">,</span> <span class="p">(</span><span class="n">image_width</span> <span class="o">-</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_result</span><span class="p">);</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_image</span><span class="p">);</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_kernel</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> </details> <p>And the Cython which has the benefit of being fast while preserving a significant part of the Python syntax.</p> <details><summary>Cython Implementation</summary> <div class="language-cython highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># cython: language_level=3
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">cimport</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">libc.math</span> <span class="kn">cimport</span> <span class="n">floor</span>

<span class="k">def</span> <span class="nf">correlate2d_cython</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="n">image</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="n">kernel</span><span class="p">):</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">image_height</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">image_width</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">kernel_height</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">kernel_width</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">result_height</span> <span class="o">=</span> <span class="n">image_height</span> <span class="o">-</span> <span class="n">kernel_height</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">result_width</span> <span class="o">=</span> <span class="n">image_width</span> <span class="o">-</span> <span class="n">kernel_width</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">cdef</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">result_height</span><span class="p">,</span> <span class="n">result_width</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">cdef</span> <span class="kt">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">result_height</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">result_width</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kernel_height</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">kernel_width</span><span class="p">):</span>
                    <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">image</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">m</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> </details> <h2 id="conclusion">Conclusion</h2> <p>In this blogpost, we focused on the architecture and implementation of the Convolutional Kernel Network, which plays with convolutional representations in RKHS. We take a look at its architecture and try to implement it <em>from scratch</em>, to better understand its ins and outs.</p> <p>To find out more, the reader is invited to read <a href="https://alberto.bietti.me/">Alberto Bietti</a>‚Äôs excellent Ph.D. thesis on the subject <d-cite key="bietti:tel-02543073"></d-cite>. In the same vein, <a href="https://dexiong.me/">Dexiong Chen</a>‚Äôs thesis deals with deep kernel methods for all types of structured data, including sequences or graphs <d-cite key="chen:tel-03193220"></d-cite>. By the way, one of the works of this Ph.D. thesis will be the subject of a future blogpost‚Ä¶ Stay tuned!</p> <h2 id="acknowledgments-"><a id="acknowledgments"></a>Acknowledgments üôèüèæ</h2> <p>We would especially like to thank Prof. <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a> for taking the time to proofread this blog post. This is all the more important to us as he is the author of the <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf">CKN paper</a>. We are very grateful to the professors of the <a href="https://mva-kernel-methods.github.io/course-2023-2024/">Kernel Methods course</a> of the <a href="https://www.master-mva.com/">MVA Master</a>: Prof. <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>, Prof. <a href="https://michaelarbel.github.io/">Michel Arbel</a>, Prof. <a href="https://jpvert.github.io/">Jean-Philippe Vert</a> and Prof. <a href="https://www.di.ens.fr/~rudi/">Alessandro Rudi</a> for introducing us to this field.</p> <p>For any further questions, please feel free to leave a comment or contact us by mail!</p>]]></content><author><name>Oussama Zekri</name></author><category term="kernel trick"/><category term="convolutional networks"/><category term="deep learning"/><category term="maths"/><category term="code"/><summary type="html"><![CDATA[Convolutional Kernel Networks]]></summary></entry></feed>