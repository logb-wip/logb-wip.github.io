
@InProceedings{pmlr-v235-ilbert24a,
  title = 	 {{SAM}former: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention},
  author =       {Ilbert, Romain and Odonnat, Ambroise and Feofanov, Vasilii and Virmaux, Aladin and Paolo, Giuseppe and Palpanas, Themis and Redko, Ievgen},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2024},
  pdf = 	 {https://arxiv.org/pdf/2402.10198},
  url = 	 {https://proceedings.mlr.press/v235/ilbert24a.html}
}

@inproceedings{foret2021sharpnessaware,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
pdf={https://arxiv.org/pdf/2010.01412},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@inproceedings{kim2022reversible,
title={Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},
author={Taesung Kim and Jinhee Kim and Yunwon Tae and Cheonbok Park and Jang-Ho Choi and Jaegul Choo},
booktitle={International Conference on Learning Representations},
year={2022},
pdf={https://openreview.net/pdf?id=cGDAkQo1C0p},
url={https://openreview.net/forum?id=cGDAkQo1C0p}
}

@misc{zhai2023sigmareparam,
title={\${\textbackslash}sigma\$Reparam: Stable Transformer Training with Spectral Reparametrization},
author={Shuangfei Zhai and Tatiana Likhomanenko and Etai Littwin and Jason Ramapuram and Dan Busbridge and Yizhe Zhang and Jiatao Gu and Joshua M. Susskind},
year={2023},
pdf={https://arxiv.org/pdf/2303.06296},
url={https://openreview.net/forum?id=QwqxO8URJzn}
}

@InProceedings{pmlr-v151-sander22a,
  title = 	 { Sinkformers: Transformers with Doubly Stochastic Attention },
  author =       {Sander, Michael E. and Ablin, Pierre and Blondel, Mathieu and Peyr\'e, Gabriel},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v151/sander22a/sander22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/sander22a.html}
}

@misc{zeng2022transformerseffectivetimeseries,
      title={Are Transformers Effective for Time Series Forecasting?}, 
      author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
      year={2022},
      pdf={https://arxiv.org/pdf/2205.13504},
      url={https://arxiv.org/abs/2205.13504}
}

@inproceedings{vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}