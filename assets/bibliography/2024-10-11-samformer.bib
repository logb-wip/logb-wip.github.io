@InProceedings{dong2021attentionrank,
  title = 	 {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
  author =       {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2793--2803},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dong21a/dong21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dong21a.html},
}

@inproceedings{anagnostidis2022signal,
title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse},
author={Sotiris Anagnostidis and Luca Biggio and Lorenzo Noci and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=FxVH7iToXS}
}

@inproceedings{brown2020fewshot,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{ilbert2024samformer,
  title = 	 {{SAM}former: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention},
  author =       {Ilbert, Romain and Odonnat, Ambroise and Feofanov, Vasilii and Virmaux, Aladin and Paolo, Giuseppe and Palpanas, Themis and Redko, Ievgen},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2024},
  pdf = 	 {https://arxiv.org/pdf/2402.10198},
  url = 	 {https://proceedings.mlr.press/v235/ilbert24a.html}
}

@inproceedings{foret2021sharpnessaware,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
pdf={https://arxiv.org/pdf/2010.01412},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@inproceedings{kim2022reversible,
title={Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},
author={Taesung Kim and Jinhee Kim and Yunwon Tae and Cheonbok Park and Jang-Ho Choi and Jaegul Choo},
booktitle={International Conference on Learning Representations},
year={2022},
pdf={https://openreview.net/pdf?id=cGDAkQo1C0p},
url={https://openreview.net/forum?id=cGDAkQo1C0p}
}

@misc{zhai2023sigmareparam,
title={\${\textbackslash}sigma\$Reparam: Stable Transformer Training with Spectral Reparametrization},
author={Shuangfei Zhai and Tatiana Likhomanenko and Etai Littwin and Jason Ramapuram and Dan Busbridge and Yizhe Zhang and Jiatao Gu and Joshua M. Susskind},
year={2023},
pdf={https://arxiv.org/pdf/2303.06296},
url={https://openreview.net/forum?id=QwqxO8URJzn}
}

@InProceedings{sander2022sinkformer,
  title = 	 { Sinkformers: Transformers with Doubly Stochastic Attention },
  author =       {Sander, Michael E. and Ablin, Pierre and Blondel, Mathieu and Peyr\'e, Gabriel},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v151/sander22a/sander22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/sander22a.html}
}

@misc{zeng2022transformerseffectivetimeseries,
      title={Are Transformers Effective for Time Series Forecasting?}, 
      author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
      year={2022},
      pdf={https://arxiv.org/pdf/2205.13504},
      url={https://arxiv.org/abs/2205.13504}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
