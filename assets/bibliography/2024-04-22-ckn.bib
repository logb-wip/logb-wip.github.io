@inproceedings{mairal2014convolutional,
 author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Convolutional Kernel Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf},
 year = {2014}
}

@inproceedings{mairal2016endtoend,
 author = {Mairal, Julien},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {End-to-End Kernel Learning with Supervised Convolutional Kernel Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},
 year = {2016}
}

@misc{bietti2022approximation,
      title={Approximation and Learning with Deep Convolutional Models: a Kernel Perspective}, 
      author={Alberto Bietti},
      year={2022},
      eprint={2102.10032},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@phdthesis{bietti:tel-02543073,
  TITLE = {Foundations of deep convolutional models through kernel methods},
  AUTHOR = {Bietti, Alberto},
  URL = {https://theses.hal.science/tel-02543073},
  NUMBER = {2019GREAM051},
  SCHOOL = {{Universit{\'e} Grenoble Alpes}},
  YEAR = {2019},
  MONTH = Nov,
  KEYWORDS = {Machine learning ; Deep learning ; Kernels ; Optimization ; Apprentissage ; Optimisation ; Noyaux ; Deep learning},
  TYPE = {Theses},
  PDF = {https://theses.hal.science/tel-02543073/file/BIETTI_2019_archivage.pdf},
  HAL_ID = {tel-02543073},
  HAL_VERSION = {v1},
}

@phdthesis{chen:tel-03193220,
  TITLE = {Structured Data Modeling with Deep Kernel Machines and Applications in Computational Biology},
  AUTHOR = {Chen, Dexiong},
  URL = {https://theses.hal.science/tel-03193220},
  NUMBER = {2020GRALM070},
  SCHOOL = {{Universit{\'e} Grenoble Alpes [2020-....]}},
  YEAR = {2020},
  MONTH = Dec,
  KEYWORDS = {Machine learning ; Kernel methods ; Bioinformatics ; Convolutional neural networks ; Sequence modeling ; Graph kernels ; Apprentissage automatique ; M{\'e}thodes {\`a} noyaux ; Bioinformatique ; R{\'e}seaux de neurones convolutifs ; Mod{\'e}lisation de s{\'e}quence ; Noyaux de graphes},
  TYPE = {Theses},
  PDF = {https://theses.hal.science/tel-03193220/file/CHEN_2020_archivage.pdf},
  HAL_ID = {tel-03193220},
  HAL_VERSION = {v1},
}

@software{Numba,
    title        = {numba/numba: Numba 0.59.1},
  author       = {Siu Kwan Lam and
                  stuartarchibald and
                  Antoine Pitrou and
                  Mark Florisson and
                  Graham Markall and
                  Stan Seibert and
                  Emergency Self-Construct and
                  Todd A. Anderson and
                  Guilherme Leobas and
                  rjenc29 and
                  Michael Collison and
                  luk-f-a and
                  Jay Bourque and
                  Kaustubh and
                  Aaron Meurer and
                  Travis E. Oliphant and
                  Nick Riasanovsky and
                  Michael Wang and
                  densmirn and
                  KrisMinchev and
                  Andre Masella and
                  Ethan Pronovost and
                  njwhite and
                  Eric Wieser and
                  Ehsan Totoni and
                  Stefan Seefeld and
                  Hernan Grecco and
                  Pearu Peterson and
                  Isaac Virshup and
                  Matty G},
  
  month        = mar,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {0.59.1},
  doi          = {10.5281/zenodo.10839385},
  url          = {https://doi.org/10.5281/zenodo.10839385}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@ARTICLE{Cython,
  author={Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
  journal={Computing in Science \& Engineering}, 
  title={Cython: The Best of Both Worlds}, 
  year={2011},
  volume={13},
  number={2},
  pages={31-39},
  keywords={Sparse matrices;Runtime;Syntactics;Computer programs;Programming;Python;Cython;numerics;scientific computing},
  doi={10.1109/MCSE.2010.118}}

@misc{spatialdropout,
      title={Efficient Object Localization Using Convolutional Networks}, 
      author={Jonathan Tompson and Ross Goroshin and Arjun Jain and Yann LeCun and Christopher Bregler},
      year={2015},
      eprint={1411.4280},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ghiasi2018dropblock,
      title={DropBlock: A regularization method for convolutional networks}, 
      author={Golnaz Ghiasi and Tsung-Yi Lin and Quoc V. Le},
      year={2018},
      eprint={1810.12890},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bietti2018group,
author = {Bietti, Alberto and Mairal, Julien},
title = {Group invariance, stability to deformations, and complexity of deep convolutional representations},
year = {2019},
issue_date = {January 2019},
abstract = {The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.},
journal = {JMLR},
url = {https://jmlr.org/papers/volume20/18-190/18-190.pdf}
}

@misc{paulin2016convolutional,
      title={Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach}, 
      author={Mattis Paulin and Julien Mairal and Matthijs Douze and Zaid Harchaoui and Florent Perronnin and Cordelia Schmid},
      year={2016},
      eprint={1603.00438},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/pdf/1603.00438}
}

@inproceedings{scholkopf2000kerneltrick,
 author = {Sch√∂lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {MIT Press},
 title = {The Kernel Trick for Distances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4e87337f366f72daa424dae11df0538c-Paper.pdf},
 year = {2000}
}

@inproceedings{aronszajn1950reproducing,
 author = {Aronszajn, Nachman},
 booktitle = {Transactions of the American Mathematical Society},
 title = {Theory of Reproducing Kernels},
 url = {https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf},
 year = {1950}
}
