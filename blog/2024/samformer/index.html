<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SAMformer | </title> <meta name="author" content=" "> <meta name="description" content="TODO"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://logb-wip.github.io/blog/2024/samformer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "SAMformer",
            "description": "TODO",
            "published": "October 11, 2024",
            "authors": [
              
              {
                "author": "Ambroise Odonnat",
                "authorURL": "https://ambroiseodt.github.io/",
                "affiliations": [
                  {
                    "name": "Huawei Noah's Ark Lab & Inria",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Oussama Zekri",
                "authorURL": "https://oussamazekri.fr",
                "affiliations": [
                  {
                    "name": "ENS Paris-Saclay & Imperial",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>SAMformer</h1> <p>TODO</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#goal">Goal üöÄ</a> </div> <div> <a href="#motivation">Motivation üîé</a> </div> <div> <a href="#samformer-%EF%B8%8F">SAMformer ‚öîÔ∏è</a> </div> <div> <a href="#getting-your-hands-dirty-%EF%B8%8F">Getting your hands dirty üñ•Ô∏è</a> </div> <div> <a href="#acknowledgments">Acknowledgments üôèüèæ</a> </div> </nav> </d-contents> <h2 id="goal-"> <a id="goal"></a>Goal üöÄ</h2> <blockquote> <p>Fear not, those who delved into the maths of the kernel trick, for its advent in deep learning is coming.</p> </blockquote> <p>In this blog post, we focus on <strong><em>SAMformer</em></strong>, a transformer-based architecture for time series forecasting proposed in <a href="https://arxiv.org/pdf/2402.10198" rel="external nofollow noopener" target="_blank"><em>SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting</em></a> <d-cite key="mairal2016endtoend"></d-cite>, one of Ambroise‚Äôs recent paper. SAMformer combines Sharpness-Aware Minimization (SAM) <d-cite key="mairal2016endtoend"></d-cite> and channel-wise attention to obtain a light-weight SOTA model with improved robustness and signal propagation compared to its competitors. This blog aims to provide a high-level view of the motivation behind SAMformer while explaining how to implement it. For the reader interested in more mathematical details or to get started with SAMformer, the paper is on <a href="https://arxiv.org/pdf/2402.10198" rel="external nofollow noopener" target="_blank">arXiv</a>, and the code can be found on <a href="https://github.com/romilbert/samformer" rel="external nofollow noopener" target="_blank">github</a>.</p> <p>1) Problem: transformers nuls en TS forecasting + very complicated and large-scale models ‚Äì&gt; hard to identify the failure. 2) We simplify transformer to only keep the key components 3) Problem identification: trainability issues 4) Possible solution: sigma reparam or SAM 5) SAM works ‚Äì&gt; putting evertyhing together</p> <h2 id="motivation-">Motivation üîé</h2> <p>‚ÄúOn va droit au but, allez voir le papier pour plus de detail.‚Äù (TO DO). Time series forecasting consists of analyzing time series data to predict future trends based on historical information. It has many applications in real-world scenarios such as forecasting ECG recordings to anticipate cardiac arrhythmia, predicting electricity consumption to match future demand, or predicting stock market prices (an exciting topic in times of inflation). Multivariate long-term forecasting is notoriously challenging due to feature correlations and long-term temporal dependencies in time series. And transformers fail at it [are transformers really effective bla bla].</p> <p>fig: meme</p> <h2 id="samformer-Ô∏è">SAMformer ‚öîÔ∏è</h2> <h3 id="trainability-issues-of-the-attention">Trainability Issues of the Attention</h3> <p>To identify the problem, we simplify the original Transformer [Vaswani et al.] to only keep the key components.</p> <p>fig: sharpeness + entropy</p> <h3 id="sam-to-the-rescue">SAM to the rescue</h3> <p>There are two possible solutions:</p> <ul> <li>$\sigma$-reparam (ref):</li> <li>SAM (ref):</li> </ul> <p>fig: intro_fig_icml</p> <h3 id="putting-everything-together">Putting Everything Together</h3> <p>Now it works on our toy example: congrats you can now solve linear regression tasks. Hum, what about true time series data? We are only one step away from the optimal architecture: add revin</p> <p>In the end, SAMformer consists of 5 layers: RevIN normalization, channel-wise attention, residual connection, linear forecasting, and RevIN denormalization. And we are SOTA:</p> <p>fig: add table and/or result figure (e.g. generalization plots with stars).</p> <h2 id="getting-your-hands-dirty-Ô∏è">Getting your hands dirty üñ•Ô∏è</h2> <p>In this section, we discuss the implementation of SAMformer.</p> <h3 id="overview">Overview</h3> <p>The original implementation of the SAMformer architecture makes use of modern deep learning frameworks such as <code class="language-plaintext highlighter-rouge">PyTorch</code> or <code class="language-plaintext highlighter-rouge">TensorFlow</code> and can be found <a href="https://github.com/romilbert/samformer" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="main-components">Main Components</h3> <p>As can be seen below, SAMformer consists of 5 layers:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/samformer_arch.png" sizes="95vw"></source> <img src="/assets/img/blog_samformer/samformer_arch.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It leads to a shallow transformer with a single head and a single encoder that can be trained with SAM <d-cite key="mairal2016endtoend"></d-cite>.</p> <p>We provide a snippet of SAMformer (few) code lines below for the interested reader.</p> <details><summary>SAMformer Implementation</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SAMFormerArchitecture</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">revin</span> <span class="o">=</span> <span class="nc">RevIN</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_keys</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_queries</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_values</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_forecaster</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1"># RevIN Normalization
</span>        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">norm</span><span class="sh">'</span><span class="p">)</span> 
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x_norm</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Channel-Wise Attention
</span>        <span class="n">queries</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_queries</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">keys</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_keys</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>        <span class="n">att_score</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Residual Connection
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">att_score</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Linear Forecasting
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_forecaster</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="c1"># RevIN Denormalization
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">denorm</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> </details> <h2 id="future-work">Future Work</h2> <p>Sigma reparam bla bla (citer Sinkformer + rank and signal propagation work on attention (attention is not all u need + signal propagation in transformer).</p> <p>fig: nuclear_norm</p> <h2 id="conclusion">Conclusion</h2> <h2 id="acknowledgments-"> <a id="acknowledgments"></a>Acknowledgments üôèüèæ</h2> <p>We thank TBD for taking the time to proofread this blog post. We thank Ambroise‚Äôs co-authors: Romain Ilbert, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, and Ievgen Redko.</p> <p>For any further questions, please feel free to leave a comment or contact us by mail!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-11-samformer.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"logb-wip/logb-wip.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 . </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>