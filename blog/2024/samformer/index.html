<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SAMformer - Efficient Time Series Forecasting with Transformers | </title> <meta name="author" content=" "> <meta name="description" content="Improved attention and optimization for better performance"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://logb-wip.github.io/blog/2024/samformer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "SAMformer - Efficient Time Series Forecasting with Transformers",
            "description": "Improved attention and optimization for better performance",
            "published": "October 11, 2024",
            "authors": [
              
              {
                "author": "Ambroise Odonnat",
                "authorURL": "https://ambroiseodt.github.io/",
                "affiliations": [
                  {
                    "name": "Noah's Ark Lab, Inria",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Oussama Zekri",
                "authorURL": "https://oussamazekri.fr",
                "affiliations": [
                  {
                    "name": "ENS Paris-Saclay",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>SAMformer - Efficient Time Series Forecasting with Transformers</h1> <p>Improved attention and optimization for better performance</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#goal">Goal üöÄ</a> </div> <div> <a href="#motivation">Motivation üîé</a> </div> <div> <a href="#samformer-%EF%B8%8F">SAMformer ‚öîÔ∏è</a> </div> <div> <a href="#getting-your-hands-dirty-%EF%B8%8F">Getting your hands dirty üñ•Ô∏è</a> </div> <div> <a href="#acknowledgments">Acknowledgments üôèüèæ</a> </div> </nav> </d-contents> <h2 id="goal-"> <a id="goal"></a>Goal üöÄ</h2> <blockquote> <p>When a simple analysis leads to an efficient implementation.</p> </blockquote> <p>In this blog post, we focus on <strong>SAMformer</strong> proposed in <a href="https://arxiv.org/pdf/2402.10198" rel="external nofollow noopener" target="_blank"><em>SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting</em></a> <d-cite key="ilbert2024samformer"></d-cite>. SAMformer combines Sharpness-Aware Minimization (SAM) <d-cite key="foret2021sharpnessaware"></d-cite> and channel-wise attention to obtain a light-weight SOTA model with improved robustness and signal propagation compared to its competitors. This blog aims to provide a high-level view of the motivation behind SAMformer while explaining how to implement it. For the reader interested in more details, the paper is on <a href="https://arxiv.org/pdf/2402.10198" rel="external nofollow noopener" target="_blank">arXiv</a>, and the code can be found on <a href="https://github.com/romilbert/samformer" rel="external nofollow noopener" target="_blank">github</a>.</p> <h2 id="motivation-">Motivation üîé</h2> <p>Time series forecasting has many applications in real-world scenarios, e.g., anticipating cardiac arrhythmia in ECG signals, predicting electricity consumption to match future demand, or forecasting stock market prices (an exciting topic in times of inflation). This is notoriously challenging, especially in multivariate and long-term settings, due to feature correlations and long-term temporal dependencies in time series. Given its success on sequential data, many variants of the original transformer have been proposed, with computationally more efficient attention layers or well-engineered decomposition schemes to deal with the temporal dependencies. This led to a wide range of complex models with many parameters. However, it has been shown that linear models perform better than those SOTA Anything-formers <d-cite key="zeng2022transformerseffectivetimeseries"></d-cite>. This came as a surprise to us given the success of the Transformer architecture in NLP <d-cite key="brown2020fewshot"></d-cite> and Computer Vision <d-cite key="foret2021sharpnessaware"></d-cite> our study right up to the development of SAMformer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/dog_meme.PNG" sizes="95vw"></source> <img src="/assets/img/blog_samformer/dog_meme.PNG" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="samformer-Ô∏è">SAMformer ‚öîÔ∏è</h2> <p>The process that led to SAMformer is akin to the good old-fashioned scientific method: empirical observations $\rightarrow$ hypothesis $\rightarrow$ experimental validation.</p> <p>1) Problem: transformers nuls en TS forecasting + very complicated and large-scale models ‚Äì&gt; hard to identify the failure. 2) We simplify transformer to only keep the key components 3) Problem identification: trainability issues 4) Possible solution: sigma reparam or SAM 5) SAM works ‚Äì&gt; putting evertyhing together 6) ‚ÄúOn va droit au but, allez voir le papier pour plus de detail.‚Äù (TO DO, something like ‚ÄúWe‚Äôll keep it concise, refer to the paper for more details.‚Äù).</p> <h3 id="trainability-issues-of-the-attention">Trainability Issues of the Attention</h3> <p>The Anything-formers are often complex and large, making pinpointing and addressing their weaknesses difficult. To identify the problem, we simplify the original Transformer <d-cite key="vaswani2017"></d-cite> to only keep the key components. Given that linear models outperformed more complex transformer ones, we naturally considered a toy problem of linear regression. The question is whether a simple transformer <em>tailored</em> to solve this task manages to do it, at least as well as a linear model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/sharpness_entropy_collapse_sam.pdf" sizes="95vw"></source> <img src="/assets/img/blog_samformer/sharpness_entropy_collapse_sam.pdf" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="sam-to-the-rescue">SAM to the rescue</h3> <p>There are two possible solutions:</p> <ul> <li>$\sigma$-reparam <d-cite key="zhai2023sigmareparam"></d-cite>:</li> <li>SAM <d-cite key="foret2021sharpnessaware"></d-cite>:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/toy_exp_losses_val_all_methods_yaxis_modif.pdf" sizes="95vw"></source> <img src="/assets/img/blog_samformer/toy_exp_losses_val_all_methods_yaxis_modif.pdf" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="putting-everything-together">Putting Everything Together</h3> <p>Now it works on our toy example: congrats you can now solve linear regression tasks. Hum, what about true time series data? We are only one step away from the optimal architecture: add revin <d-cite key="kim2022reversible"></d-cite></p> <p>In the end, SAMformer consists of 5 layers: RevIN normalization, channel-wise attention, residual connection, linear forecasting, and RevIN denormalization. And we are SOTA:</p> <p>fig: add table and/or result figure (e.g. generalization plots with stars).</p> <h2 id="getting-your-hands-dirty-Ô∏è">Getting your hands dirty üñ•Ô∏è</h2> <p>In this section, we discuss the implementation of SAMformer which makes use of modern deep learning frameworks such as <code class="language-plaintext highlighter-rouge">PyTorch</code> or <code class="language-plaintext highlighter-rouge">TensorFlow</code> and can be found <a href="https://github.com/romilbert/samformer" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="main-components">Main Components</h3> <p>As can be seen below, SAMformer consists of 5 layers:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/samformer_arch.png" sizes="95vw"></source> <img src="/assets/img/blog_samformer/samformer_arch.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It leads to a shallow transformer with a single head and a single encoder that can be trained with SAM <d-cite key="foret2021sharpnessaware"></d-cite>.</p> <p>We provide a snippet of SAMformer (few) code lines below for the interested reader.</p> <details><summary>SAMformer Implementation</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SAMFormerArchitecture</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">revin</span> <span class="o">=</span> <span class="nc">RevIN</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_keys</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_queries</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">compute_values</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear_forecaster</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">pred_horizon</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1"># RevIN Normalization
</span>        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">norm</span><span class="sh">'</span><span class="p">)</span> 
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x_norm</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Channel-Wise Attention
</span>        <span class="n">queries</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_queries</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">keys</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_keys</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, hid_dim)
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_values</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>        <span class="n">att_score</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Residual Connection
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">att_score</span> <span class="c1"># (n, D, L)
</span>
        <span class="c1"># Linear Forecasting
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_forecaster</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="c1"># RevIN Denormalization
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">revin</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">denorm</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (n, D, H)
</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> </details> <h2 id="future-work">Future Work</h2> <p>While studying the trainability issues of the Transformer, we stumbled over the fact that the entropy collapse appeared in tandem with a sharp loss. However, we observed that the entropy collapse was benign, i.e., solving it did not improve the performance that much. This is different from the conclusions on text and images found in <d-cite key="zhai2023sigmareparam"></d-cite>. Moreover, we saw that using $\sigma$-reparam <d-cite key="zhai2023sigmareparam"></d-cite> decreases the signal propagation a lot, up to the point of having almost uniform attention. This leads to rank collapse, which is known to appear in attention-based models and to impact the generalization performance <d-cite key="anagnostidis2022signal"></d-cite> <d-cite key="dong2021attentionrank"></d-cite>. Finally, we manage to demonstrate that, indeed, $\sigma$-reparam induces a rank collapse. Formally, $\sigma$-reparam aims to minimize blabla, which in turn can be used to upper-bound the rank of the attention internal computations. We have</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_samformer/nuclear_norm.png" sizes="95vw"></source> <img src="/assets/img/blog_samformer/nuclear_norm.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Sigma reparam bla bla (citer Sinkformer <d-cite key="sander2022sinkformer"></d-cite> + rank and signal propagation work on attention (attention is not all u need + signal propagation in transformer).</p> <h2 id="conclusion">Conclusion</h2> <p>This blog post has presented Ambroise‚Äôs recent work on transformers for time series forecasting. By integrating SAM with channel-wise attention, SAMformer achieves state-of-the-art performance with a lightweight and robust design, making it a superior choice for time series forecasting. While there are many models and now foundation models for time series forecasting, SAMformer has the benefit of simplicity and efficiency. Beyond the model itself, we felt it was important to share the research process that led to it in a world of rapid changes and extreme development of algorithms that are performance-oriented. While SAMformer outperforms SOTA models (at the time) for a fraction of the computational cost, our goal was primarily to better understand the failure modes of previous methods. We hope this kind of methodology can inspire researchers and practitioners towards more efficient and reliable methods.</p> <p>Finally, we want to acknowledge and thank the open-source community. In particular, this work would not have been possible without access to the code and papers of SAM[], $\sigma$-reparam[] or to the machine learning libraries such as PyTorch, TensorFlow, or Scikit-Learn. For the reader interested in more mathematical details or getting started with SAMformer, the paper is on <a href="https://arxiv.org/pdf/2402.10198" rel="external nofollow noopener" target="_blank">arXiv</a>, and the code can be found on <a href="https://github.com/romilbert/samformer" rel="external nofollow noopener" target="_blank">github</a>.</p> <h2 id="acknowledgments-"> <a id="acknowledgments"></a>Acknowledgments üôèüèæ</h2> <p>We thank TBD for taking the time to proofread this blog post. Ambroise thanks all his co-authors, without whom SAMformer <d-cite key="ilbert2024samformer"></d-cite> wouldn‚Äôt exist, and in particular, his Ph.D. supervisor, Ievgen Redko, for the freedom and trust he provided during this project.</p> <p>For any further questions, please feel free to leave a comment or contact us by mail!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-11-samformer.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"logb-wip/logb-wip.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 . </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>